{% extends "layout.html" %} {% block body %}

<div class="row">
  <div class="column-head">
    <h2>NTRODUCTION</h2>
    <br>
    <p>
      Sign language serves as the primary mode of commu-nication for individuals with hearing disabilities, enabling
      them to
      express their thoughts, emotions, and ideas. However, the limited understanding of sign language among the general
      population
      poses significant challenges for effective communication between individuals who primarily use sign language and
      those
      who do not understand it. Bridging this communication gap is crucial for fostering inclusivity and equal
      participation for
      individuals with hearing disabilities in various social and professional contexts.
      This research aims to improve sign language translation systems to facilitate seamless communication between
      individuals with hearing disabilities and the wider community. Specifically, we focus on comparing the performance
      of
      two machinelearning architectures, LSTM (Long Short-Term Memory) and CNN (Convolutional Neural Network), in
      translating
      sign language gestures.
      Utilizing the Python programming language and leveraging powerful libraries such as Mediapipe and OpenCV, we
      collect
      sign language gesture data through imitation and recording.
      Our analysis reveals that the LSTM model excels in capturing the temporal dynamics and nuances of sign language
      gestures, making it well-suited for detecting and interpreting actions within the gestures. However, the LSTM
      model faces
      challenges in accurately translating complex multi-word expressions, which are common in sign language
      communication.
      To address these challenges and enhance the accuracy of sign language translation, we conduct a comprehensive
      comparative analysis of LSTM and CNN architectures. By evaluating their performance on multi-word translation
      tasks,
      we aim to identify the strengths and weaknesses of each approach. Factors such as translation accuracy, robustness
      to variations
      in gestures, computational efficiency, and real-time translation capability will be considered in our analysis.
      The outcomes of our research will contribute to the development of more accurate and efficient sign language
      translation systems, paving the way for improved communication and inclusivity for individuals with hearing
      disabilities.
      By bridging the gap between sign language users and the wider community, we aim to foster understanding, equal
      participation, and empowerment for individuals with hearing disabilities.</p>
  </div>
</div>
<div class="row">
  <div class="column-model">
    <h2>CNN</h2>
    <p>(Click picture to use)</p>
    <br>
    <a href="/cnn"><img src="../static/dataimage/cnn.png" alt="cnn"></a>
    <div class="button-container">
      <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network" target="_blank" class="button-style">What is CNN?</a>
    </div>
  </div>
  <div class="column-model">
    <h2>LSTM</h2>
    <p>(Click picture to use)</p>
    <br>
    <a href="/lstm"><img src="../static/dataimage/lstm.png" alt="lstm"></a>
    <div class="button-container">
      <a href="https://en.wikipedia.org/wiki/Long_short-term_memory" target="_blank" class="button-style">What is
        LSTM?</a>
    </div>
  </div>
</div>

{% endblock %}